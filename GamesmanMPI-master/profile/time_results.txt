Beginning testing for test_games/four_to_one.py from 4 to 12, 3 tests each

Testing with 4 processes
---------
[Toms-MacBook-Pro-2.local:77773] mca_base_component_repository_open: unable to open mca_grpcomm_bad: dlopen(/usr/local/lib/openmpi/mca_grpcomm_bad.so, 9): Symbol not found: _orte_grpcomm_base_modex
  Referenced from: /usr/local/lib/openmpi/mca_grpcomm_bad.so
  Expected in: flat namespace
 in /usr/local/lib/openmpi/mca_grpcomm_bad.so (ignored)
[Toms-MacBook-Pro-2.local:77773] mca_base_component_repository_open: ras "/usr/local/lib/openmpi/mca_ras_gridengine" uses an MCA interface that is not recognized (component MCA v2.0.0 != supported MCA v2.1.0) -- ignored
[Toms-MacBook-Pro-2.local:77774] mca: base: component_find: shmem "mca_shmem_mmap" uses an MCA interface that is not recognized (component MCA v2.1.0 != supported MCA v2.0.0) -- ignored
[Toms-MacBook-Pro-2.local:77774] mca: base: component_find: shmem "mca_shmem_posix" uses an MCA interface that is not recognized (component MCA v2.1.0 != supported MCA v2.0.0) -- ignored
[Toms-MacBook-Pro-2.local:77774] mca: base: component_find: shmem "mca_shmem_sysv" uses an MCA interface that is not recognized (component MCA v2.1.0 != supported MCA v2.0.0) -- ignored
--------------------------------------------------------------------------
It looks like opal_init failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during opal_init; some of which are due to configuration or
environment problems.  This failure appears to be an internal failure;
here's some additional information (which may only be relevant to an
Open MPI developer):

  opal_shmem_base_select failed
  --> Returned value -1 instead of OPAL_SUCCESS
--------------------------------------------------------------------------
--------------------------------------------------------------------------
It looks like orte_init failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during orte_init; some of which are due to configuration or
environment problems.  This failure appears to be an internal failure;
here's some additional information (which may only be relevant to an
Open MPI developer):

  opal_init failed
  --> Returned value Error (-1) instead of ORTE_SUCCESS
--------------------------------------------------------------------------
--------------------------------------------------------------------------
It looks like MPI_INIT failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during MPI_INIT; some of which are due to configuration or environment
problems.  This failure appears to be an internal failure; here's some
additional information (which may only be relevant to an Open MPI
developer):

  ompi_mpi_init: ompi_rte_init failed
  --> Returned "Error" (-1) instead of "Success" (0)
--------------------------------------------------------------------------
*** An error occurred in MPI_Init_thread
*** on a NULL communicator
*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
***    and potentially your MPI job)
[Toms-MacBook-Pro-2.local:77774] Local abort before MPI_INIT completed successfully; not able to aggregate error messages, and not able to guarantee that all other processes were killed!

real	0m0.084s
user	0m0.106s
sys	0m0.076s

Testing with 4 processes
---------
[Toms-MacBook-Pro-2.local:77779] mca_base_component_repository_open: unable to open mca_grpcomm_bad: dlopen(/usr/local/lib/openmpi/mca_grpcomm_bad.so, 9): Symbol not found: _orte_grpcomm_base_modex
  Referenced from: /usr/local/lib/openmpi/mca_grpcomm_bad.so
  Expected in: flat namespace
 in /usr/local/lib/openmpi/mca_grpcomm_bad.so (ignored)
[Toms-MacBook-Pro-2.local:77779] mca_base_component_repository_open: ras "/usr/local/lib/openmpi/mca_ras_gridengine" uses an MCA interface that is not recognized (component MCA v2.0.0 != supported MCA v2.1.0) -- ignored
--------------------------------------------------------------------------
It looks like orte_init failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during orte_init; some of which are due to configuration or
environment problems.  This failure appears to be an internal failure;
here's some additional information (which may only be relevant to an
Open MPI developer):

  opal_init failed
  --> Returned value Error (-1) instead of ORTE_SUCCESS
--------------------------------------------------------------------------
*** An error occurred in MPI_Init_thread
*** on a NULL communicator
*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
***    and potentially your MPI job)
[Toms-MacBook-Pro-2.local:77780] Local abort before MPI_INIT completed successfully; not able to aggregate error messages, and not able to guarantee that all other processes were killed!
[Toms-MacBook-Pro-2.local:77780] mca: base: component_find: shmem "mca_shmem_mmap" uses an MCA interface that is not recognized (component MCA v2.1.0 != supported MCA v2.0.0) -- ignored
[Toms-MacBook-Pro-2.local:77780] mca: base: component_find: shmem "mca_shmem_posix" uses an MCA interface that is not recognized (component MCA v2.1.0 != supported MCA v2.0.0) -- ignored
[Toms-MacBook-Pro-2.local:77780] mca: base: component_find: shmem "mca_shmem_sysv" uses an MCA interface that is not recognized (component MCA v2.1.0 != supported MCA v2.0.0) -- ignored
--------------------------------------------------------------------------
It looks like opal_init failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during opal_init; some of which are due to configuration or
environment problems.  This failure appears to be an internal failure;
here's some additional information (which may only be relevant to an
Open MPI developer):

  opal_shmem_base_select failed
  --> Returned value -1 instead of OPAL_SUCCESS
--------------------------------------------------------------------------
--------------------------------------------------------------------------
It looks like MPI_INIT failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during MPI_INIT; some of which are due to configuration or environment
problems.  This failure appears to be an internal failure; here's some
additional information (which may only be relevant to an Open MPI
developer):

  ompi_mpi_init: ompi_rte_init failed
  --> Returned "Error" (-1) instead of "Success" (0)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
It looks like orte_init failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during orte_init; some of which are due to configuration or
environment problems.  This failure appears to be an internal failure;
here's some additional information (which may only be relevant to an
Open MPI developer):

  opal_init failed
  --> Returned value Error (-1) instead of ORTE_SUCCESS
--------------------------------------------------------------------------
*** An error occurred in MPI_Init_thread
*** on a NULL communicator
*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
***    and potentially your MPI job)
[Toms-MacBook-Pro-2.local:77781] Local abort before MPI_INIT completed successfully; not able to aggregate error messages, and not able to guarantee that all other processes were killed!
[Toms-MacBook-Pro-2.local:77781] mca: base: component_find: shmem "mca_shmem_mmap" uses an MCA interface that is not recognized (component MCA v2.1.0 != supported MCA v2.0.0) -- ignored
[Toms-MacBook-Pro-2.local:77781] mca: base: component_find: shmem "mca_shmem_posix" uses an MCA interface that is not recognized (component MCA v2.1.0 != supported MCA v2.0.0) -- ignored
[Toms-MacBook-Pro-2.local:77781] mca: base: component_find: shmem "mca_shmem_sysv" uses an MCA interface that is not recognized (component MCA v2.1.0 != supported MCA v2.0.0) -- ignored
--------------------------------------------------------------------------
It looks like opal_init failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during opal_init; some of which are due to configuration or
environment problems.  This failure appears to be an internal failure;
here's some additional information (which may only be relevant to an
Open MPI developer):

  opal_shmem_base_select failed
  --> Returned value -1 instead of OPAL_SUCCESS
--------------------------------------------------------------------------
--------------------------------------------------------------------------
It looks like MPI_INIT failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during MPI_INIT; some of which are due to configuration or environment
problems.  This failure appears to be an internal failure; here's some
additional information (which may only be relevant to an Open MPI
developer):

  ompi_mpi_init: ompi_rte_init failed
  --> Returned "Error" (-1) instead of "Success" (0)
--------------------------------------------------------------------------

real	0m0.072s
user	0m0.097s
sys	0m0.071s

Testing with 4 processes
---------
[Toms-MacBook-Pro-2.local:77785] mca_base_component_repository_open: unable to open mca_grpcomm_bad: dlopen(/usr/local/lib/openmpi/mca_grpcomm_bad.so, 9): Symbol not found: _orte_grpcomm_base_modex
  Referenced from: /usr/local/lib/openmpi/mca_grpcomm_bad.so
  Expected in: flat namespace
 in /usr/local/lib/openmpi/mca_grpcomm_bad.so (ignored)
[Toms-MacBook-Pro-2.local:77785] mca_base_component_repository_open: ras "/usr/local/lib/openmpi/mca_ras_gridengine" uses an MCA interface that is not recognized (component MCA v2.0.0 != supported MCA v2.1.0) -- ignored
[Toms-MacBook-Pro-2.local:77786] mca: base: component_find: shmem "mca_shmem_mmap" uses an MCA interface that is not recognized (component MCA v2.1.0 != supported MCA v2.0.0) -- ignored
[Toms-MacBook-Pro-2.local:77786] mca: base: component_find: shmem "mca_shmem_posix" uses an MCA interface that is not recognized (component MCA v2.1.0 != supported MCA v2.0.0) -- ignored
[Toms-MacBook-Pro-2.local:77786] mca: base: component_find: shmem "mca_shmem_sysv" uses an MCA interface that is not recognized (component MCA v2.1.0 != supported MCA v2.0.0) -- ignored
--------------------------------------------------------------------------
It looks like opal_init failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during opal_init; some of which are due to configuration or
environment problems.  This failure appears to be an internal failure;
here's some additional information (which may only be relevant to an
Open MPI developer):

  opal_shmem_base_select failed
  --> Returned value -1 instead of OPAL_SUCCESS
--------------------------------------------------------------------------
--------------------------------------------------------------------------
It looks like orte_init failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during orte_init; some of which are due to configuration or
environment problems.  This failure appears to be an internal failure;
here's some additional information (which may only be relevant to an
Open MPI developer):

  opal_init failed
  --> Returned value Error (-1) instead of ORTE_SUCCESS
--------------------------------------------------------------------------
--------------------------------------------------------------------------
It looks like MPI_INIT failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during MPI_INIT; some of which are due to configuration or environment
problems.  This failure appears to be an internal failure; here's some
additional information (which may only be relevant to an Open MPI
developer):

  ompi_mpi_init: ompi_rte_init failed
  --> Returned "Error" (-1) instead of "Success" (0)
--------------------------------------------------------------------------
*** An error occurred in MPI_Init_thread
*** on a NULL communicator
*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
***    and potentially your MPI job)
[Toms-MacBook-Pro-2.local:77786] Local abort before MPI_INIT completed successfully; not able to aggregate error messages, and not able to guarantee that all other processes were killed!

real	0m0.069s
user	0m0.092s
sys	0m0.068s

Testing with 5 processes
---------
[Toms-MacBook-Pro-2.local:77794] mca_base_component_repository_open: unable to open mca_grpcomm_bad: dlopen(/usr/local/lib/openmpi/mca_grpcomm_bad.so, 9): Symbol not found: _orte_grpcomm_base_modex
  Referenced from: /usr/local/lib/openmpi/mca_grpcomm_bad.so
  Expected in: flat namespace
 in /usr/local/lib/openmpi/mca_grpcomm_bad.so (ignored)
[Toms-MacBook-Pro-2.local:77794] mca_base_component_repository_open: ras "/usr/local/lib/openmpi/mca_ras_gridengine" uses an MCA interface that is not recognized (component MCA v2.0.0 != supported MCA v2.1.0) -- ignored
--------------------------------------------------------------------------
It looks like orte_init failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during orte_init; some of which are due to configuration or
environment problems.  This failure appears to be an internal failure;
here's some additional information (which may only be relevant to an
Open MPI developer):

  opal_init failed
  --> Returned value Error (-1) instead of ORTE_SUCCESS
--------------------------------------------------------------------------
*** An error occurred in MPI_Init_thread
*** on a NULL communicator
*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
***    and potentially your MPI job)
[Toms-MacBook-Pro-2.local:77795] Local abort before MPI_INIT completed successfully; not able to aggregate error messages, and not able to guarantee that all other processes were killed!
[Toms-MacBook-Pro-2.local:77795] mca: base: component_find: shmem "mca_shmem_mmap" uses an MCA interface that is not recognized (component MCA v2.1.0 != supported MCA v2.0.0) -- ignored
[Toms-MacBook-Pro-2.local:77795] mca: base: component_find: shmem "mca_shmem_posix" uses an MCA interface that is not recognized (component MCA v2.1.0 != supported MCA v2.0.0) -- ignored
[Toms-MacBook-Pro-2.local:77795] mca: base: component_find: shmem "mca_shmem_sysv" uses an MCA interface that is not recognized (component MCA v2.1.0 != supported MCA v2.0.0) -- ignored
--------------------------------------------------------------------------
It looks like opal_init failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during opal_init; some of which are due to configuration or
environment problems.  This failure appears to be an internal failure;
here's some additional information (which may only be relevant to an
Open MPI developer):

  opal_shmem_base_select failed
  --> Returned value -1 instead of OPAL_SUCCESS
--------------------------------------------------------------------------
--------------------------------------------------------------------------
It looks like MPI_INIT failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during MPI_INIT; some of which are due to configuration or environment
problems.  This failure appears to be an internal failure; here's some
additional information (which may only be relevant to an Open MPI
developer):

  ompi_mpi_init: ompi_rte_init failed
  --> Returned "Error" (-1) instead of "Success" (0)
--------------------------------------------------------------------------

real	0m0.078s
user	0m0.117s
sys	0m0.094s

Testing with 5 processes
---------
[Toms-MacBook-Pro-2.local:77801] mca_base_component_repository_open: unable to open mca_grpcomm_bad: dlopen(/usr/local/lib/openmpi/mca_grpcomm_bad.so, 9): Symbol not found: _orte_grpcomm_base_modex
  Referenced from: /usr/local/lib/openmpi/mca_grpcomm_bad.so
  Expected in: flat namespace
 in /usr/local/lib/openmpi/mca_grpcomm_bad.so (ignored)
[Toms-MacBook-Pro-2.local:77801] mca_base_component_repository_open: ras "/usr/local/lib/openmpi/mca_ras_gridengine" uses an MCA interface that is not recognized (component MCA v2.0.0 != supported MCA v2.1.0) -- ignored
[Toms-MacBook-Pro-2.local:77802] mca: base: component_find: shmem "mca_shmem_mmap" uses an MCA interface that is not recognized (component MCA v2.1.0 != supported MCA v2.0.0) -- ignored
[Toms-MacBook-Pro-2.local:77802] mca: base: component_find: shmem "mca_shmem_posix" uses an MCA interface that is not recognized (component MCA v2.1.0 != supported MCA v2.0.0) -- ignored
[Toms-MacBook-Pro-2.local:77802] mca: base: component_find: shmem "mca_shmem_sysv" uses an MCA interface that is not recognized (component MCA v2.1.0 != supported MCA v2.0.0) -- ignored
--------------------------------------------------------------------------
It looks like opal_init failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during opal_init; some of which are due to configuration or
environment problems.  This failure appears to be an internal failure;
here's some additional information (which may only be relevant to an
Open MPI developer):

  opal_shmem_base_select failed
  --> Returned value -1 instead of OPAL_SUCCESS
--------------------------------------------------------------------------
--------------------------------------------------------------------------
It looks like orte_init failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during orte_init; some of which are due to configuration or
environment problems.  This failure appears to be an internal failure;
here's some additional information (which may only be relevant to an
Open MPI developer):

  opal_init failed
  --> Returned value Error (-1) instead of ORTE_SUCCESS
--------------------------------------------------------------------------
*** An error occurred in MPI_Init_thread
*** on a NULL communicator
*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
***    and potentially your MPI job)
[Toms-MacBook-Pro-2.local:77802] Local abort before MPI_INIT completed successfully; not able to aggregate error messages, and not able to guarantee that all other processes were killed!
--------------------------------------------------------------------------
It looks like MPI_INIT failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during MPI_INIT; some of which are due to configuration or environment
problems.  This failure appears to be an internal failure; here's some
additional information (which may only be relevant to an Open MPI
developer):

  ompi_mpi_init: ompi_rte_init failed
  --> Returned "Error" (-1) instead of "Success" (0)
--------------------------------------------------------------------------

real	0m0.078s
user	0m0.121s
sys	0m0.093s

Testing with 5 processes
---------
[Toms-MacBook-Pro-2.local:77808] mca_base_component_repository_open: unable to open mca_grpcomm_bad: dlopen(/usr/local/lib/openmpi/mca_grpcomm_bad.so, 9): Symbol not found: _orte_grpcomm_base_modex
  Referenced from: /usr/local/lib/openmpi/mca_grpcomm_bad.so
  Expected in: flat namespace
 in /usr/local/lib/openmpi/mca_grpcomm_bad.so (ignored)
[Toms-MacBook-Pro-2.local:77808] mca_base_component_repository_open: ras "/usr/local/lib/openmpi/mca_ras_gridengine" uses an MCA interface that is not recognized (component MCA v2.0.0 != supported MCA v2.1.0) -- ignored
[Toms-MacBook-Pro-2.local:77810] mca: base: component_find: shmem "mca_shmem_mmap" uses an MCA interface that is not recognized (component MCA v2.1.0 != supported MCA v2.0.0) -- ignored
[Toms-MacBook-Pro-2.local:77810] mca: base: component_find: shmem "mca_shmem_posix" uses an MCA interface that is not recognized (component MCA v2.1.0 != supported MCA v2.0.0) -- ignored
[Toms-MacBook-Pro-2.local:77810] mca: base: component_find: shmem "mca_shmem_sysv" uses an MCA interface that is not recognized (component MCA v2.1.0 != supported MCA v2.0.0) -- ignored
--------------------------------------------------------------------------
It looks like opal_init failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during opal_init; some of which are due to configuration or
environment problems.  This failure appears to be an internal failure;
here's some additional information (which may only be relevant to an
Open MPI developer):

  opal_shmem_base_select failed
  --> Returned value -1 instead of OPAL_SUCCESS
--------------------------------------------------------------------------
--------------------------------------------------------------------------
It looks like orte_init failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during orte_init; some of which are due to configuration or
environment problems.  This failure appears to be an internal failure;
here's some additional information (which may only be relevant to an
Open MPI developer):

  opal_init failed
  --> Returned value Error (-1) instead of ORTE_SUCCESS
--------------------------------------------------------------------------
--------------------------------------------------------------------------
It looks like MPI_INIT failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during MPI_INIT; some of which are due to configuration or environment
problems.  This failure appears to be an internal failure; here's some
additional information (which may only be relevant to an Open MPI
developer):

  ompi_mpi_init: ompi_rte_init failed
  --> Returned "Error" (-1) instead of "Success" (0)
--------------------------------------------------------------------------
*** An error occurred in MPI_Init_thread
*** on a NULL communicator
*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
***    and potentially your MPI job)
[Toms-MacBook-Pro-2.local:77810] Local abort before MPI_INIT completed successfully; not able to aggregate error messages, and not able to guarantee that all other processes were killed!

real	0m0.080s
user	0m0.127s
sys	0m0.093s

Testing with 6 processes
---------
[Toms-MacBook-Pro-2.local:77818] mca_base_component_repository_open: unable to open mca_grpcomm_bad: dlopen(/usr/local/lib/openmpi/mca_grpcomm_bad.so, 9): Symbol not found: _orte_grpcomm_base_modex
  Referenced from: /usr/local/lib/openmpi/mca_grpcomm_bad.so
  Expected in: flat namespace
 in /usr/local/lib/openmpi/mca_grpcomm_bad.so (ignored)
[Toms-MacBook-Pro-2.local:77818] mca_base_component_repository_open: ras "/usr/local/lib/openmpi/mca_ras_gridengine" uses an MCA interface that is not recognized (component MCA v2.0.0 != supported MCA v2.1.0) -- ignored
--------------------------------------------------------------------------
It looks like orte_init failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during orte_init; some of which are due to configuration or
environment problems.  This failure appears to be an internal failure;
here's some additional information (which may only be relevant to an
Open MPI developer):

  opal_init failed
  --> Returned value Error (-1) instead of ORTE_SUCCESS
--------------------------------------------------------------------------
*** An error occurred in MPI_Init_thread
*** on a NULL communicator
*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
***    and potentially your MPI job)
[Toms-MacBook-Pro-2.local:77819] Local abort before MPI_INIT completed successfully; not able to aggregate error messages, and not able to guarantee that all other processes were killed!
[Toms-MacBook-Pro-2.local:77819] mca: base: component_find: shmem "mca_shmem_mmap" uses an MCA interface that is not recognized (component MCA v2.1.0 != supported MCA v2.0.0) -- ignored
[Toms-MacBook-Pro-2.local:77819] mca: base: component_find: shmem "mca_shmem_posix" uses an MCA interface that is not recognized (component MCA v2.1.0 != supported MCA v2.0.0) -- ignored
[Toms-MacBook-Pro-2.local:77819] mca: base: component_find: shmem "mca_shmem_sysv" uses an MCA interface that is not recognized (component MCA v2.1.0 != supported MCA v2.0.0) -- ignored
--------------------------------------------------------------------------
It looks like opal_init failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during opal_init; some of which are due to configuration or
environment problems.  This failure appears to be an internal failure;
here's some additional information (which may only be relevant to an
Open MPI developer):

  opal_shmem_base_select failed
  --> Returned value -1 instead of OPAL_SUCCESS
--------------------------------------------------------------------------
--------------------------------------------------------------------------
It looks like MPI_INIT failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during MPI_INIT; some of which are due to configuration or environment
problems.  This failure appears to be an internal failure; here's some
additional information (which may only be relevant to an Open MPI
developer):

  ompi_mpi_init: ompi_rte_init failed
  --> Returned "Error" (-1) instead of "Success" (0)
--------------------------------------------------------------------------

real	0m0.081s
user	0m0.133s
sys	0m0.113s

Testing with 6 processes
---------
[Toms-MacBook-Pro-2.local:77826] mca_base_component_repository_open: unable to open mca_grpcomm_bad: dlopen(/usr/local/lib/openmpi/mca_grpcomm_bad.so, 9): Symbol not found: _orte_grpcomm_base_modex
  Referenced from: /usr/local/lib/openmpi/mca_grpcomm_bad.so
  Expected in: flat namespace
 in /usr/local/lib/openmpi/mca_grpcomm_bad.so (ignored)
[Toms-MacBook-Pro-2.local:77826] mca_base_component_repository_open: ras "/usr/local/lib/openmpi/mca_ras_gridengine" uses an MCA interface that is not recognized (component MCA v2.0.0 != supported MCA v2.1.0) -- ignored
[Toms-MacBook-Pro-2.local:77827] mca: base: component_find: shmem "mca_shmem_mmap" uses an MCA interface that is not recognized (component MCA v2.1.0 != supported MCA v2.0.0) -- ignored
[Toms-MacBook-Pro-2.local:77827] mca: base: component_find: shmem "mca_shmem_posix" uses an MCA interface that is not recognized (component MCA v2.1.0 != supported MCA v2.0.0) -- ignored
[Toms-MacBook-Pro-2.local:77827] mca: base: component_find: shmem "mca_shmem_sysv" uses an MCA interface that is not recognized (component MCA v2.1.0 != supported MCA v2.0.0) -- ignored
--------------------------------------------------------------------------
It looks like opal_init failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during opal_init; some of which are due to configuration or
environment problems.  This failure appears to be an internal failure;
here's some additional information (which may only be relevant to an
Open MPI developer):

  opal_shmem_base_select failed
  --> Returned value -1 instead of OPAL_SUCCESS
--------------------------------------------------------------------------
--------------------------------------------------------------------------
It looks like orte_init failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during orte_init; some of which are due to configuration or
environment problems.  This failure appears to be an internal failure;
here's some additional information (which may only be relevant to an
Open MPI developer):

  opal_init failed
  --> Returned value Error (-1) instead of ORTE_SUCCESS
--------------------------------------------------------------------------
--------------------------------------------------------------------------
It looks like MPI_INIT failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during MPI_INIT; some of which are due to configuration or environment
problems.  This failure appears to be an internal failure; here's some
additional information (which may only be relevant to an Open MPI
developer):

  ompi_mpi_init: ompi_rte_init failed
  --> Returned "Error" (-1) instead of "Success" (0)
--------------------------------------------------------------------------
*** An error occurred in MPI_Init_thread
*** on a NULL communicator
*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
***    and potentially your MPI job)
[Toms-MacBook-Pro-2.local:77827] Local abort before MPI_INIT completed successfully; not able to aggregate error messages, and not able to guarantee that all other processes were killed!

real	0m0.081s
user	0m0.135s
sys	0m0.113s

Testing with 6 processes
---------
[Toms-MacBook-Pro-2.local:77834] mca_base_component_repository_open: unable to open mca_grpcomm_bad: dlopen(/usr/local/lib/openmpi/mca_grpcomm_bad.so, 9): Symbol not found: _orte_grpcomm_base_modex
  Referenced from: /usr/local/lib/openmpi/mca_grpcomm_bad.so
  Expected in: flat namespace
 in /usr/local/lib/openmpi/mca_grpcomm_bad.so (ignored)
[Toms-MacBook-Pro-2.local:77834] mca_base_component_repository_open: ras "/usr/local/lib/openmpi/mca_ras_gridengine" uses an MCA interface that is not recognized (component MCA v2.0.0 != supported MCA v2.1.0) -- ignored
[Toms-MacBook-Pro-2.local:77835] mca: base: component_find: shmem "mca_shmem_mmap" uses an MCA interface that is not recognized (component MCA v2.1.0 != supported MCA v2.0.0) -- ignored
[Toms-MacBook-Pro-2.local:77835] mca: base: component_find: shmem "mca_shmem_posix" uses an MCA interface that is not recognized (component MCA v2.1.0 != supported MCA v2.0.0) -- ignored
[Toms-MacBook-Pro-2.local:77835] mca: base: component_find: shmem "mca_shmem_sysv" uses an MCA interface that is not recognized (component MCA v2.1.0 != supported MCA v2.0.0) -- ignored
--------------------------------------------------------------------------
It looks like opal_init failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during opal_init; some of which are due to configuration or
environment problems.  This failure appears to be an internal failure;
here's some additional information (which may only be relevant to an
Open MPI developer):

  opal_shmem_base_select failed
  --> Returned value -1 instead of OPAL_SUCCESS
--------------------------------------------------------------------------
--------------------------------------------------------------------------
It looks like orte_init failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during orte_init; some of which are due to configuration or
environment problems.  This failure appears to be an internal failure;
here's some additional information (which may only be relevant to an
Open MPI developer):

  opal_init failed
  --> Returned value Error (-1) instead of ORTE_SUCCESS
--------------------------------------------------------------------------
--------------------------------------------------------------------------
It looks like MPI_INIT failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during MPI_INIT; some of which are due to configuration or environment
problems.  This failure appears to be an internal failure; here's some
additional information (which may only be relevant to an Open MPI
developer):

  ompi_mpi_init: ompi_rte_init failed
  --> Returned "Error" (-1) instead of "Success" (0)
--------------------------------------------------------------------------
*** An error occurred in MPI_Init_thread
*** on a NULL communicator
*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
***    and potentially your MPI job)
[Toms-MacBook-Pro-2.local:77835] Local abort before MPI_INIT completed successfully; not able to aggregate error messages, and not able to guarantee that all other processes were killed!

real	0m0.080s
user	0m0.120s
sys	0m0.108s

Testing with 7 processes
---------
[Toms-MacBook-Pro-2.local:77845] mca_base_component_repository_open: unable to open mca_grpcomm_bad: dlopen(/usr/local/lib/openmpi/mca_grpcomm_bad.so, 9): Symbol not found: _orte_grpcomm_base_modex
  Referenced from: /usr/local/lib/openmpi/mca_grpcomm_bad.so
  Expected in: flat namespace
 in /usr/local/lib/openmpi/mca_grpcomm_bad.so (ignored)
[Toms-MacBook-Pro-2.local:77845] mca_base_component_repository_open: ras "/usr/local/lib/openmpi/mca_ras_gridengine" uses an MCA interface that is not recognized (component MCA v2.0.0 != supported MCA v2.1.0) -- ignored
--------------------------------------------------------------------------
It looks like orte_init failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during orte_init; some of which are due to configuration or
environment problems.  This failure appears to be an internal failure;
here's some additional information (which may only be relevant to an
Open MPI developer):

  opal_init failed
  --> Returned value Error (-1) instead of ORTE_SUCCESS
--------------------------------------------------------------------------
*** An error occurred in MPI_Init_thread
*** on a NULL communicator
*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
***    and potentially your MPI job)
[Toms-MacBook-Pro-2.local:77846] Local abort before MPI_INIT completed successfully; not able to aggregate error messages, and not able to guarantee that all other processes were killed!
[Toms-MacBook-Pro-2.local:77846] mca: base: component_find: shmem "mca_shmem_mmap" uses an MCA interface that is not recognized (component MCA v2.1.0 != supported MCA v2.0.0) -- ignored
[Toms-MacBook-Pro-2.local:77846] mca: base: component_find: shmem "mca_shmem_posix" uses an MCA interface that is not recognized (component MCA v2.1.0 != supported MCA v2.0.0) -- ignored
[Toms-MacBook-Pro-2.local:77846] mca: base: component_find: shmem "mca_shmem_sysv" uses an MCA interface that is not recognized (component MCA v2.1.0 != supported MCA v2.0.0) -- ignored
--------------------------------------------------------------------------
It looks like opal_init failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during opal_init; some of which are due to configuration or
environment problems.  This failure appears to be an internal failure;
here's some additional information (which may only be relevant to an
Open MPI developer):

  opal_shmem_base_select failed
  --> Returned value -1 instead of OPAL_SUCCESS
--------------------------------------------------------------------------
--------------------------------------------------------------------------
It looks like MPI_INIT failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during MPI_INIT; some of which are due to configuration or environment
problems.  This failure appears to be an internal failure; here's some
additional information (which may only be relevant to an Open MPI
developer):

  ompi_mpi_init: ompi_rte_init failed
  --> Returned "Error" (-1) instead of "Success" (0)
--------------------------------------------------------------------------

real	0m0.093s
user	0m0.159s
sys	0m0.129s

Testing with 7 processes
---------
[Toms-MacBook-Pro-2.local:77854] mca_base_component_repository_open: unable to open mca_grpcomm_bad: dlopen(/usr/local/lib/openmpi/mca_grpcomm_bad.so, 9): Symbol not found: _orte_grpcomm_base_modex
  Referenced from: /usr/local/lib/openmpi/mca_grpcomm_bad.so
  Expected in: flat namespace
 in /usr/local/lib/openmpi/mca_grpcomm_bad.so (ignored)
[Toms-MacBook-Pro-2.local:77854] mca_base_component_repository_open: ras "/usr/local/lib/openmpi/mca_ras_gridengine" uses an MCA interface that is not recognized (component MCA v2.0.0 != supported MCA v2.1.0) -- ignored
[Toms-MacBook-Pro-2.local:77855] mca: base: component_find: shmem "mca_shmem_mmap" uses an MCA interface that is not recognized (component MCA v2.1.0 != supported MCA v2.0.0) -- ignored
[Toms-MacBook-Pro-2.local:77855] mca: base: component_find: shmem "mca_shmem_posix" uses an MCA interface that is not recognized (component MCA v2.1.0 != supported MCA v2.0.0) -- ignored
[Toms-MacBook-Pro-2.local:77855] mca: base: component_find: shmem "mca_shmem_sysv" uses an MCA interface that is not recognized (component MCA v2.1.0 != supported MCA v2.0.0) -- ignored
--------------------------------------------------------------------------
It looks like opal_init failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during opal_init; some of which are due to configuration or
environment problems.  This failure appears to be an internal failure;
here's some additional information (which may only be relevant to an
Open MPI developer):

  opal_shmem_base_select failed
  --> Returned value -1 instead of OPAL_SUCCESS
--------------------------------------------------------------------------
--------------------------------------------------------------------------
It looks like orte_init failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during orte_init; some of which are due to configuration or
environment problems.  This failure appears to be an internal failure;
here's some additional information (which may only be relevant to an
Open MPI developer):

  opal_init failed
  --> Returned value Error (-1) instead of ORTE_SUCCESS
--------------------------------------------------------------------------
--------------------------------------------------------------------------
It looks like MPI_INIT failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during MPI_INIT; some of which are due to configuration or environment
problems.  This failure appears to be an internal failure; here's some
additional information (which may only be relevant to an Open MPI
developer):

  ompi_mpi_init: ompi_rte_init failed
  --> Returned "Error" (-1) instead of "Success" (0)
--------------------------------------------------------------------------
*** An error occurred in MPI_Init_thread
*** on a NULL communicator
*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
***    and potentially your MPI job)
[Toms-MacBook-Pro-2.local:77855] Local abort before MPI_INIT completed successfully; not able to aggregate error messages, and not able to guarantee that all other processes were killed!

real	0m0.091s
user	0m0.178s
sys	0m0.135s

Testing with 7 processes
---------
[Toms-MacBook-Pro-2.local:77863] mca_base_component_repository_open: unable to open mca_grpcomm_bad: dlopen(/usr/local/lib/openmpi/mca_grpcomm_bad.so, 9): Symbol not found: _orte_grpcomm_base_modex
  Referenced from: /usr/local/lib/openmpi/mca_grpcomm_bad.so
  Expected in: flat namespace
 in /usr/local/lib/openmpi/mca_grpcomm_bad.so (ignored)
[Toms-MacBook-Pro-2.local:77863] mca_base_component_repository_open: ras "/usr/local/lib/openmpi/mca_ras_gridengine" uses an MCA interface that is not recognized (component MCA v2.0.0 != supported MCA v2.1.0) -- ignored
[Toms-MacBook-Pro-2.local:77864] mca: base: component_find: shmem "mca_shmem_mmap" uses an MCA interface that is not recognized (component MCA v2.1.0 != supported MCA v2.0.0) -- ignored
[Toms-MacBook-Pro-2.local:77864] mca: base: component_find: shmem "mca_shmem_posix" uses an MCA interface that is not recognized (component MCA v2.1.0 != supported MCA v2.0.0) -- ignored
[Toms-MacBook-Pro-2.local:77864] mca: base: component_find: shmem "mca_shmem_sysv" uses an MCA interface that is not recognized (component MCA v2.1.0 != supported MCA v2.0.0) -- ignored
--------------------------------------------------------------------------
It looks like opal_init failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during opal_init; some of which are due to configuration or
environment problems.  This failure appears to be an internal failure;
here's some additional information (which may only be relevant to an
Open MPI developer):

  opal_shmem_base_select failed
  --> Returned value -1 instead of OPAL_SUCCESS
--------------------------------------------------------------------------
--------------------------------------------------------------------------
It looks like orte_init failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during orte_init; some of which are due to configuration or
environment problems.  This failure appears to be an internal failure;
here's some additional information (which may only be relevant to an
Open MPI developer):

  opal_init failed
  --> Returned value Error (-1) instead of ORTE_SUCCESS
--------------------------------------------------------------------------
*** An error occurred in MPI_Init_thread
*** on a NULL communicator
*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
***    and potentially your MPI job)
[Toms-MacBook-Pro-2.local:77864] Local abort before MPI_INIT completed successfully; not able to aggregate error messages, and not able to guarantee that all other processes were killed!
--------------------------------------------------------------------------
It looks like MPI_INIT failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during MPI_INIT; some of which are due to configuration or environment
problems.  This failure appears to be an internal failure; here's some
additional information (which may only be relevant to an Open MPI
developer):

  ompi_mpi_init: ompi_rte_init failed
  --> Returned "Error" (-1) instead of "Success" (0)
--------------------------------------------------------------------------
[Toms-MacBook-Pro-2.local:77866] mca: base: component_find: shmem "mca_shmem_mmap" uses an MCA interface that is not recognized (component MCA v2.1.0 != supported MCA v2.0.0) -- ignored
[Toms-MacBook-Pro-2.local:77866] mca: base: component_find: shmem "mca_shmem_posix" uses an MCA interface that is not recognized (component MCA v2.1.0 != supported MCA v2.0.0) -- ignored

real	0m0.092s
user	0m0.186s
sys	0m0.131s

Testing with 8 processes
---------
[Toms-MacBook-Pro-2.local:77875] mca_base_component_repository_open: unable to open mca_grpcomm_bad: dlopen(/usr/local/lib/openmpi/mca_grpcomm_bad.so, 9): Symbol not found: _orte_grpcomm_base_modex
  Referenced from: /usr/local/lib/openmpi/mca_grpcomm_bad.so
  Expected in: flat namespace
 in /usr/local/lib/openmpi/mca_grpcomm_bad.so (ignored)
[Toms-MacBook-Pro-2.local:77875] mca_base_component_repository_open: ras "/usr/local/lib/openmpi/mca_ras_gridengine" uses an MCA interface that is not recognized (component MCA v2.0.0 != supported MCA v2.1.0) -- ignored
[Toms-MacBook-Pro-2.local:77876] mca: base: component_find: shmem "mca_shmem_mmap" uses an MCA interface that is not recognized (component MCA v2.1.0 != supported MCA v2.0.0) -- ignored
[Toms-MacBook-Pro-2.local:77876] mca: base: component_find: shmem "mca_shmem_posix" uses an MCA interface that is not recognized (component MCA v2.1.0 != supported MCA v2.0.0) -- ignored
[Toms-MacBook-Pro-2.local:77876] mca: base: component_find: shmem "mca_shmem_sysv" uses an MCA interface that is not recognized (component MCA v2.1.0 != supported MCA v2.0.0) -- ignored
--------------------------------------------------------------------------
It looks like opal_init failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during opal_init; some of which are due to configuration or
environment problems.  This failure appears to be an internal failure;
here's some additional information (which may only be relevant to an
Open MPI developer):

  opal_shmem_base_select failed
  --> Returned value -1 instead of OPAL_SUCCESS
--------------------------------------------------------------------------
--------------------------------------------------------------------------
It looks like MPI_INIT failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during MPI_INIT; some of which are due to configuration or environment
problems.  This failure appears to be an internal failure; here's some
additional information (which may only be relevant to an Open MPI
developer):

  ompi_mpi_init: ompi_rte_init failed
  --> Returned "Error" (-1) instead of "Success" (0)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
It looks like orte_init failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during orte_init; some of which are due to configuration or
environment problems.  This failure appears to be an internal failure;
here's some additional information (which may only be relevant to an
Open MPI developer):

  opal_init failed
  --> Returned value Error (-1) instead of ORTE_SUCCESS
--------------------------------------------------------------------------
*** An error occurred in MPI_Init_thread
*** on a NULL communicator
*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
***    and potentially your MPI job)
[Toms-MacBook-Pro-2.local:77876] Local abort before MPI_INIT completed successfully; not able to aggregate error messages, and not able to guarantee that all other processes were killed!

real	0m0.086s
user	0m0.142s
sys	0m0.143s

Testing with 8 processes
---------
[Toms-MacBook-Pro-2.local:77885] mca_base_component_repository_open: unable to open mca_grpcomm_bad: dlopen(/usr/local/lib/openmpi/mca_grpcomm_bad.so, 9): Symbol not found: _orte_grpcomm_base_modex
  Referenced from: /usr/local/lib/openmpi/mca_grpcomm_bad.so
  Expected in: flat namespace
 in /usr/local/lib/openmpi/mca_grpcomm_bad.so (ignored)
[Toms-MacBook-Pro-2.local:77885] mca_base_component_repository_open: ras "/usr/local/lib/openmpi/mca_ras_gridengine" uses an MCA interface that is not recognized (component MCA v2.0.0 != supported MCA v2.1.0) -- ignored
[Toms-MacBook-Pro-2.local:77886] mca: base: component_find: shmem "mca_shmem_mmap" uses an MCA interface that is not recognized (component MCA v2.1.0 != supported MCA v2.0.0) -- ignored
[Toms-MacBook-Pro-2.local:77886] mca: base: component_find: shmem "mca_shmem_posix" uses an MCA interface that is not recognized (component MCA v2.1.0 != supported MCA v2.0.0) -- ignored
[Toms-MacBook-Pro-2.local:77886] mca: base: component_find: shmem "mca_shmem_sysv" uses an MCA interface that is not recognized (component MCA v2.1.0 != supported MCA v2.0.0) -- ignored
--------------------------------------------------------------------------
It looks like opal_init failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during opal_init; some of which are due to configuration or
environment problems.  This failure appears to be an internal failure;
here's some additional information (which may only be relevant to an
Open MPI developer):

  opal_shmem_base_select failed
  --> Returned value -1 instead of OPAL_SUCCESS
--------------------------------------------------------------------------
--------------------------------------------------------------------------
It looks like orte_init failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during orte_init; some of which are due to configuration or
environment problems.  This failure appears to be an internal failure;
here's some additional information (which may only be relevant to an
Open MPI developer):

  opal_init failed
  --> Returned value Error (-1) instead of ORTE_SUCCESS
--------------------------------------------------------------------------
*** An error occurred in MPI_Init_thread
*** on a NULL communicator
*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
***    and potentially your MPI job)
[Toms-MacBook-Pro-2.local:77886] Local abort before MPI_INIT completed successfully; not able to aggregate error messages, and not able to guarantee that all other processes were killed!
--------------------------------------------------------------------------
It looks like MPI_INIT failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during MPI_INIT; some of which are due to configuration or environment
problems.  This failure appears to be an internal failure; here's some
additional information (which may only be relevant to an Open MPI
developer):

  ompi_mpi_init: ompi_rte_init failed
  --> Returned "Error" (-1) instead of "Success" (0)
--------------------------------------------------------------------------

real	0m0.083s
user	0m0.132s
sys	0m0.138s

Testing with 8 processes
---------
[Toms-MacBook-Pro-2.local:77895] mca_base_component_repository_open: unable to open mca_grpcomm_bad: dlopen(/usr/local/lib/openmpi/mca_grpcomm_bad.so, 9): Symbol not found: _orte_grpcomm_base_modex
  Referenced from: /usr/local/lib/openmpi/mca_grpcomm_bad.so
  Expected in: flat namespace
 in /usr/local/lib/openmpi/mca_grpcomm_bad.so (ignored)
[Toms-MacBook-Pro-2.local:77895] mca_base_component_repository_open: ras "/usr/local/lib/openmpi/mca_ras_gridengine" uses an MCA interface that is not recognized (component MCA v2.0.0 != supported MCA v2.1.0) -- ignored
[Toms-MacBook-Pro-2.local:77896] mca: base: component_find: shmem "mca_shmem_mmap" uses an MCA interface that is not recognized (component MCA v2.1.0 != supported MCA v2.0.0) -- ignored
[Toms-MacBook-Pro-2.local:77896] mca: base: component_find: shmem "mca_shmem_posix" uses an MCA interface that is not recognized (component MCA v2.1.0 != supported MCA v2.0.0) -- ignored
[Toms-MacBook-Pro-2.local:77896] mca: base: component_find: shmem "mca_shmem_sysv" uses an MCA interface that is not recognized (component MCA v2.1.0 != supported MCA v2.0.0) -- ignored
--------------------------------------------------------------------------
It looks like opal_init failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during opal_init; some of which are due to configuration or
environment problems.  This failure appears to be an internal failure;
here's some additional information (which may only be relevant to an
Open MPI developer):

  opal_shmem_base_select failed
  --> Returned value -1 instead of OPAL_SUCCESS
--------------------------------------------------------------------------
--------------------------------------------------------------------------
It looks like orte_init failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during orte_init; some of which are due to configuration or
environment problems.  This failure appears to be an internal failure;
here's some additional information (which may only be relevant to an
Open MPI developer):

  opal_init failed
  --> Returned value Error (-1) instead of ORTE_SUCCESS
--------------------------------------------------------------------------
--------------------------------------------------------------------------
It looks like MPI_INIT failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during MPI_INIT; some of which are due to configuration or environment
problems.  This failure appears to be an internal failure; here's some
additional information (which may only be relevant to an Open MPI
developer):

  ompi_mpi_init: ompi_rte_init failed
  --> Returned "Error" (-1) instead of "Success" (0)
--------------------------------------------------------------------------
*** An error occurred in MPI_Init_thread
*** on a NULL communicator
*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
***    and potentially your MPI job)
[Toms-MacBook-Pro-2.local:77896] Local abort before MPI_INIT completed successfully; not able to aggregate error messages, and not able to guarantee that all other processes were killed!

real	0m0.085s
user	0m0.135s
sys	0m0.145s

Testing with 9 processes
---------
[Toms-MacBook-Pro-2.local:77908] mca_base_component_repository_open: unable to open mca_grpcomm_bad: dlopen(/usr/local/lib/openmpi/mca_grpcomm_bad.so, 9): Symbol not found: _orte_grpcomm_base_modex
  Referenced from: /usr/local/lib/openmpi/mca_grpcomm_bad.so
  Expected in: flat namespace
 in /usr/local/lib/openmpi/mca_grpcomm_bad.so (ignored)
[Toms-MacBook-Pro-2.local:77908] mca_base_component_repository_open: ras "/usr/local/lib/openmpi/mca_ras_gridengine" uses an MCA interface that is not recognized (component MCA v2.0.0 != supported MCA v2.1.0) -- ignored
[Toms-MacBook-Pro-2.local:77909] mca: base: component_find: shmem "mca_shmem_mmap" uses an MCA interface that is not recognized (component MCA v2.1.0 != supported MCA v2.0.0) -- ignored
[Toms-MacBook-Pro-2.local:77909] mca: base: component_find: shmem "mca_shmem_posix" uses an MCA interface that is not recognized (component MCA v2.1.0 != supported MCA v2.0.0) -- ignored
[Toms-MacBook-Pro-2.local:77909] mca: base: component_find: shmem "mca_shmem_sysv" uses an MCA interface that is not recognized (component MCA v2.1.0 != supported MCA v2.0.0) -- ignored
--------------------------------------------------------------------------
It looks like opal_init failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during opal_init; some of which are due to configuration or
environment problems.  This failure appears to be an internal failure;
here's some additional information (which may only be relevant to an
Open MPI developer):

  opal_shmem_base_select failed
  --> Returned value -1 instead of OPAL_SUCCESS
--------------------------------------------------------------------------
--------------------------------------------------------------------------
It looks like orte_init failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during orte_init; some of which are due to configuration or
environment problems.  This failure appears to be an internal failure;
here's some additional information (which may only be relevant to an
Open MPI developer):

  opal_init failed
  --> Returned value Error (-1) instead of ORTE_SUCCESS
--------------------------------------------------------------------------
*** An error occurred in MPI_Init_thread
*** on a NULL communicator
*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
***    and potentially your MPI job)
[Toms-MacBook-Pro-2.local:77909] Local abort before MPI_INIT completed successfully; not able to aggregate error messages, and not able to guarantee that all other processes were killed!
--------------------------------------------------------------------------
It looks like MPI_INIT failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during MPI_INIT; some of which are due to configuration or environment
problems.  This failure appears to be an internal failure; here's some
additional information (which may only be relevant to an Open MPI
developer):

  ompi_mpi_init: ompi_rte_init failed
  --> Returned "Error" (-1) instead of "Success" (0)
--------------------------------------------------------------------------

real	0m0.082s
user	0m0.131s
sys	0m0.141s

Testing with 9 processes
---------
[Toms-MacBook-Pro-2.local:77919] mca_base_component_repository_open: unable to open mca_grpcomm_bad: dlopen(/usr/local/lib/openmpi/mca_grpcomm_bad.so, 9): Symbol not found: _orte_grpcomm_base_modex
  Referenced from: /usr/local/lib/openmpi/mca_grpcomm_bad.so
  Expected in: flat namespace
 in /usr/local/lib/openmpi/mca_grpcomm_bad.so (ignored)
[Toms-MacBook-Pro-2.local:77919] mca_base_component_repository_open: ras "/usr/local/lib/openmpi/mca_ras_gridengine" uses an MCA interface that is not recognized (component MCA v2.0.0 != supported MCA v2.1.0) -- ignored
[Toms-MacBook-Pro-2.local:77920] mca: base: component_find: shmem "mca_shmem_mmap" uses an MCA interface that is not recognized (component MCA v2.1.0 != supported MCA v2.0.0) -- ignored
[Toms-MacBook-Pro-2.local:77920] mca: base: component_find: shmem "mca_shmem_posix" uses an MCA interface that is not recognized (component MCA v2.1.0 != supported MCA v2.0.0) -- ignored
[Toms-MacBook-Pro-2.local:77920] mca: base: component_find: shmem "mca_shmem_sysv" uses an MCA interface that is not recognized (component MCA v2.1.0 != supported MCA v2.0.0) -- ignored
--------------------------------------------------------------------------
It looks like opal_init failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during opal_init; some of which are due to configuration or
environment problems.  This failure appears to be an internal failure;
here's some additional information (which may only be relevant to an
Open MPI developer):

  opal_shmem_base_select failed
  --> Returned value -1 instead of OPAL_SUCCESS
--------------------------------------------------------------------------
--------------------------------------------------------------------------
It looks like orte_init failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during orte_init; some of which are due to configuration or
environment problems.  This failure appears to be an internal failure;
here's some additional information (which may only be relevant to an
Open MPI developer):

  opal_init failed
  --> Returned value Error (-1) instead of ORTE_SUCCESS
--------------------------------------------------------------------------
--------------------------------------------------------------------------
It looks like MPI_INIT failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during MPI_INIT; some of which are due to configuration or environment
problems.  This failure appears to be an internal failure; here's some
additional information (which may only be relevant to an Open MPI
developer):

  ompi_mpi_init: ompi_rte_init failed
  --> Returned "Error" (-1) instead of "Success" (0)
--------------------------------------------------------------------------
*** An error occurred in MPI_Init_thread
*** on a NULL communicator
*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
***    and potentially your MPI job)
[Toms-MacBook-Pro-2.local:77920] Local abort before MPI_INIT completed successfully; not able to aggregate error messages, and not able to guarantee that all other processes were killed!

real	0m0.084s
user	0m0.134s
sys	0m0.142s

Testing with 9 processes
---------
[Toms-MacBook-Pro-2.local:77930] mca_base_component_repository_open: unable to open mca_grpcomm_bad: dlopen(/usr/local/lib/openmpi/mca_grpcomm_bad.so, 9): Symbol not found: _orte_grpcomm_base_modex
  Referenced from: /usr/local/lib/openmpi/mca_grpcomm_bad.so
  Expected in: flat namespace
 in /usr/local/lib/openmpi/mca_grpcomm_bad.so (ignored)
[Toms-MacBook-Pro-2.local:77930] mca_base_component_repository_open: ras "/usr/local/lib/openmpi/mca_ras_gridengine" uses an MCA interface that is not recognized (component MCA v2.0.0 != supported MCA v2.1.0) -- ignored
[Toms-MacBook-Pro-2.local:77931] mca: base: component_find: shmem "mca_shmem_mmap" uses an MCA interface that is not recognized (component MCA v2.1.0 != supported MCA v2.0.0) -- ignored
[Toms-MacBook-Pro-2.local:77931] mca: base: component_find: shmem "mca_shmem_posix" uses an MCA interface that is not recognized (component MCA v2.1.0 != supported MCA v2.0.0) -- ignored
[Toms-MacBook-Pro-2.local:77931] mca: base: component_find: shmem "mca_shmem_sysv" uses an MCA interface that is not recognized (component MCA v2.1.0 != supported MCA v2.0.0) -- ignored
--------------------------------------------------------------------------
It looks like opal_init failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during opal_init; some of which are due to configuration or
environment problems.  This failure appears to be an internal failure;
here's some additional information (which may only be relevant to an
Open MPI developer):

  opal_shmem_base_select failed
  --> Returned value -1 instead of OPAL_SUCCESS
--------------------------------------------------------------------------
--------------------------------------------------------------------------
It looks like orte_init failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during orte_init; some of which are due to configuration or
environment problems.  This failure appears to be an internal failure;
here's some additional information (which may only be relevant to an
Open MPI developer):

  opal_init failed
  --> Returned value Error (-1) instead of ORTE_SUCCESS
--------------------------------------------------------------------------
--------------------------------------------------------------------------
It looks like MPI_INIT failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during MPI_INIT; some of which are due to configuration or environment
problems.  This failure appears to be an internal failure; here's some
additional information (which may only be relevant to an Open MPI
developer):

  ompi_mpi_init: ompi_rte_init failed
  --> Returned "Error" (-1) instead of "Success" (0)
--------------------------------------------------------------------------
*** An error occurred in MPI_Init_thread
*** on a NULL communicator
*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
***    and potentially your MPI job)
[Toms-MacBook-Pro-2.local:77931] Local abort before MPI_INIT completed successfully; not able to aggregate error messages, and not able to guarantee that all other processes were killed!

real	0m0.083s
user	0m0.135s
sys	0m0.141s

Testing with 10 processes
---------
[Toms-MacBook-Pro-2.local:77944] mca_base_component_repository_open: unable to open mca_grpcomm_bad: dlopen(/usr/local/lib/openmpi/mca_grpcomm_bad.so, 9): Symbol not found: _orte_grpcomm_base_modex
  Referenced from: /usr/local/lib/openmpi/mca_grpcomm_bad.so
  Expected in: flat namespace
 in /usr/local/lib/openmpi/mca_grpcomm_bad.so (ignored)
[Toms-MacBook-Pro-2.local:77944] mca_base_component_repository_open: ras "/usr/local/lib/openmpi/mca_ras_gridengine" uses an MCA interface that is not recognized (component MCA v2.0.0 != supported MCA v2.1.0) -- ignored
[Toms-MacBook-Pro-2.local:77945] mca: base: component_find: shmem "mca_shmem_mmap" uses an MCA interface that is not recognized (component MCA v2.1.0 != supported MCA v2.0.0) -- ignored
[Toms-MacBook-Pro-2.local:77945] mca: base: component_find: shmem "mca_shmem_posix" uses an MCA interface that is not recognized (component MCA v2.1.0 != supported MCA v2.0.0) -- ignored
[Toms-MacBook-Pro-2.local:77945] mca: base: component_find: shmem "mca_shmem_sysv" uses an MCA interface that is not recognized (component MCA v2.1.0 != supported MCA v2.0.0) -- ignored
--------------------------------------------------------------------------
It looks like opal_init failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during opal_init; some of which are due to configuration or
environment problems.  This failure appears to be an internal failure;
here's some additional information (which may only be relevant to an
Open MPI developer):

  opal_shmem_base_select failed
  --> Returned value -1 instead of OPAL_SUCCESS
--------------------------------------------------------------------------
--------------------------------------------------------------------------
It looks like orte_init failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during orte_init; some of which are due to configuration or
environment problems.  This failure appears to be an internal failure;
here's some additional information (which may only be relevant to an
Open MPI developer):

  opal_init failed
  --> Returned value Error (-1) instead of ORTE_SUCCESS
--------------------------------------------------------------------------
*** An error occurred in MPI_Init_thread
*** on a NULL communicator
*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
***    and potentially your MPI job)
--------------------------------------------------------------------------
It looks like MPI_INIT failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during MPI_INIT; some of which are due to configuration or environment
problems.  This failure appears to be an internal failure; here's some
additional information (which may only be relevant to an Open MPI
developer):

  ompi_mpi_init: ompi_rte_init failed
  --> Returned "Error" (-1) instead of "Success" (0)
--------------------------------------------------------------------------
[Toms-MacBook-Pro-2.local:77945] Local abort before MPI_INIT completed successfully; not able to aggregate error messages, and not able to guarantee that all other processes were killed!

real	0m0.087s
user	0m0.156s
sys	0m0.144s

Testing with 10 processes
---------
[Toms-MacBook-Pro-2.local:77956] mca_base_component_repository_open: unable to open mca_grpcomm_bad: dlopen(/usr/local/lib/openmpi/mca_grpcomm_bad.so, 9): Symbol not found: _orte_grpcomm_base_modex
  Referenced from: /usr/local/lib/openmpi/mca_grpcomm_bad.so
  Expected in: flat namespace
 in /usr/local/lib/openmpi/mca_grpcomm_bad.so (ignored)
[Toms-MacBook-Pro-2.local:77956] mca_base_component_repository_open: ras "/usr/local/lib/openmpi/mca_ras_gridengine" uses an MCA interface that is not recognized (component MCA v2.0.0 != supported MCA v2.1.0) -- ignored
[Toms-MacBook-Pro-2.local:77957] mca: base: component_find: shmem "mca_shmem_mmap" uses an MCA interface that is not recognized (component MCA v2.1.0 != supported MCA v2.0.0) -- ignored
[Toms-MacBook-Pro-2.local:77957] mca: base: component_find: shmem "mca_shmem_posix" uses an MCA interface that is not recognized (component MCA v2.1.0 != supported MCA v2.0.0) -- ignored
[Toms-MacBook-Pro-2.local:77957] mca: base: component_find: shmem "mca_shmem_sysv" uses an MCA interface that is not recognized (component MCA v2.1.0 != supported MCA v2.0.0) -- ignored
--------------------------------------------------------------------------
It looks like opal_init failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during opal_init; some of which are due to configuration or
environment problems.  This failure appears to be an internal failure;
here's some additional information (which may only be relevant to an
Open MPI developer):

  opal_shmem_base_select failed
  --> Returned value -1 instead of OPAL_SUCCESS
--------------------------------------------------------------------------
--------------------------------------------------------------------------
It looks like orte_init failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during orte_init; some of which are due to configuration or
environment problems.  This failure appears to be an internal failure;
here's some additional information (which may only be relevant to an
Open MPI developer):

  opal_init failed
  --> Returned value Error (-1) instead of ORTE_SUCCESS
--------------------------------------------------------------------------
*** An error occurred in MPI_Init_thread
*** on a NULL communicator
*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
***    and potentially your MPI job)
[Toms-MacBook-Pro-2.local:77957] Local abort before MPI_INIT completed successfully; not able to aggregate error messages, and not able to guarantee that all other processes were killed!
--------------------------------------------------------------------------
It looks like MPI_INIT failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during MPI_INIT; some of which are due to configuration or environment
problems.  This failure appears to be an internal failure; here's some
additional information (which may only be relevant to an Open MPI
developer):

  ompi_mpi_init: ompi_rte_init failed
  --> Returned "Error" (-1) instead of "Success" (0)
--------------------------------------------------------------------------

real	0m0.080s
user	0m0.131s
sys	0m0.128s

Testing with 10 processes
---------
[Toms-MacBook-Pro-2.local:77968] mca_base_component_repository_open: unable to open mca_grpcomm_bad: dlopen(/usr/local/lib/openmpi/mca_grpcomm_bad.so, 9): Symbol not found: _orte_grpcomm_base_modex
  Referenced from: /usr/local/lib/openmpi/mca_grpcomm_bad.so
  Expected in: flat namespace
 in /usr/local/lib/openmpi/mca_grpcomm_bad.so (ignored)
[Toms-MacBook-Pro-2.local:77968] mca_base_component_repository_open: ras "/usr/local/lib/openmpi/mca_ras_gridengine" uses an MCA interface that is not recognized (component MCA v2.0.0 != supported MCA v2.1.0) -- ignored
[Toms-MacBook-Pro-2.local:77969] mca: base: component_find: shmem "mca_shmem_mmap" uses an MCA interface that is not recognized (component MCA v2.1.0 != supported MCA v2.0.0) -- ignored
[Toms-MacBook-Pro-2.local:77969] mca: base: component_find: shmem "mca_shmem_posix" uses an MCA interface that is not recognized (component MCA v2.1.0 != supported MCA v2.0.0) -- ignored
[Toms-MacBook-Pro-2.local:77969] mca: base: component_find: shmem "mca_shmem_sysv" uses an MCA interface that is not recognized (component MCA v2.1.0 != supported MCA v2.0.0) -- ignored
--------------------------------------------------------------------------
It looks like opal_init failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during opal_init; some of which are due to configuration or
environment problems.  This failure appears to be an internal failure;
here's some additional information (which may only be relevant to an
Open MPI developer):

  opal_shmem_base_select failed
  --> Returned value -1 instead of OPAL_SUCCESS
--------------------------------------------------------------------------
--------------------------------------------------------------------------
It looks like orte_init failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during orte_init; some of which are due to configuration or
environment problems.  This failure appears to be an internal failure;
here's some additional information (which may only be relevant to an
Open MPI developer):

  opal_init failed
  --> Returned value Error (-1) instead of ORTE_SUCCESS
--------------------------------------------------------------------------
*** An error occurred in MPI_Init_thread
*** on a NULL communicator
*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
***    and potentially your MPI job)
[Toms-MacBook-Pro-2.local:77969] Local abort before MPI_INIT completed successfully; not able to aggregate error messages, and not able to guarantee that all other processes were killed!
--------------------------------------------------------------------------
It looks like MPI_INIT failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during MPI_INIT; some of which are due to configuration or environment
problems.  This failure appears to be an internal failure; here's some
additional information (which may only be relevant to an Open MPI
developer):

  ompi_mpi_init: ompi_rte_init failed
  --> Returned "Error" (-1) instead of "Success" (0)
--------------------------------------------------------------------------

real	0m0.082s
user	0m0.136s
sys	0m0.138s

Testing with 11 processes
---------
[Toms-MacBook-Pro-2.local:77983] mca_base_component_repository_open: unable to open mca_grpcomm_bad: dlopen(/usr/local/lib/openmpi/mca_grpcomm_bad.so, 9): Symbol not found: _orte_grpcomm_base_modex
  Referenced from: /usr/local/lib/openmpi/mca_grpcomm_bad.so
  Expected in: flat namespace
 in /usr/local/lib/openmpi/mca_grpcomm_bad.so (ignored)
[Toms-MacBook-Pro-2.local:77983] mca_base_component_repository_open: ras "/usr/local/lib/openmpi/mca_ras_gridengine" uses an MCA interface that is not recognized (component MCA v2.0.0 != supported MCA v2.1.0) -- ignored
[Toms-MacBook-Pro-2.local:77984] mca: base: component_find: shmem "mca_shmem_mmap" uses an MCA interface that is not recognized (component MCA v2.1.0 != supported MCA v2.0.0) -- ignored
[Toms-MacBook-Pro-2.local:77984] mca: base: component_find: shmem "mca_shmem_posix" uses an MCA interface that is not recognized (component MCA v2.1.0 != supported MCA v2.0.0) -- ignored
[Toms-MacBook-Pro-2.local:77984] mca: base: component_find: shmem "mca_shmem_sysv" uses an MCA interface that is not recognized (component MCA v2.1.0 != supported MCA v2.0.0) -- ignored
--------------------------------------------------------------------------
It looks like opal_init failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during opal_init; some of which are due to configuration or
environment problems.  This failure appears to be an internal failure;
here's some additional information (which may only be relevant to an
Open MPI developer):

  opal_shmem_base_select failed
  --> Returned value -1 instead of OPAL_SUCCESS
--------------------------------------------------------------------------
--------------------------------------------------------------------------
It looks like orte_init failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during orte_init; some of which are due to configuration or
environment problems.  This failure appears to be an internal failure;
here's some additional information (which may only be relevant to an
Open MPI developer):

  opal_init failed
  --> Returned value Error (-1) instead of ORTE_SUCCESS
--------------------------------------------------------------------------
--------------------------------------------------------------------------
It looks like MPI_INIT failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during MPI_INIT; some of which are due to configuration or environment
problems.  This failure appears to be an internal failure; here's some
additional information (which may only be relevant to an Open MPI
developer):

  ompi_mpi_init: ompi_rte_init failed
  --> Returned "Error" (-1) instead of "Success" (0)
--------------------------------------------------------------------------
*** An error occurred in MPI_Init_thread
*** on a NULL communicator
*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
***    and potentially your MPI job)
[Toms-MacBook-Pro-2.local:77984] Local abort before MPI_INIT completed successfully; not able to aggregate error messages, and not able to guarantee that all other processes were killed!

real	0m0.085s
user	0m0.140s
sys	0m0.144s

Testing with 11 processes
---------
[Toms-MacBook-Pro-2.local:77996] mca_base_component_repository_open: unable to open mca_grpcomm_bad: dlopen(/usr/local/lib/openmpi/mca_grpcomm_bad.so, 9): Symbol not found: _orte_grpcomm_base_modex
  Referenced from: /usr/local/lib/openmpi/mca_grpcomm_bad.so
  Expected in: flat namespace
 in /usr/local/lib/openmpi/mca_grpcomm_bad.so (ignored)
[Toms-MacBook-Pro-2.local:77996] mca_base_component_repository_open: ras "/usr/local/lib/openmpi/mca_ras_gridengine" uses an MCA interface that is not recognized (component MCA v2.0.0 != supported MCA v2.1.0) -- ignored
--------------------------------------------------------------------------
It looks like orte_init failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during orte_init; some of which are due to configuration or
environment problems.  This failure appears to be an internal failure;
here's some additional information (which may only be relevant to an
Open MPI developer):

  opal_init failed
  --> Returned value Error (-1) instead of ORTE_SUCCESS
--------------------------------------------------------------------------
*** An error occurred in MPI_Init_thread
*** on a NULL communicator
*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
***    and potentially your MPI job)
[Toms-MacBook-Pro-2.local:77997] Local abort before MPI_INIT completed successfully; not able to aggregate error messages, and not able to guarantee that all other processes were killed!
[Toms-MacBook-Pro-2.local:77997] mca: base: component_find: shmem "mca_shmem_mmap" uses an MCA interface that is not recognized (component MCA v2.1.0 != supported MCA v2.0.0) -- ignored
[Toms-MacBook-Pro-2.local:77997] mca: base: component_find: shmem "mca_shmem_posix" uses an MCA interface that is not recognized (component MCA v2.1.0 != supported MCA v2.0.0) -- ignored
[Toms-MacBook-Pro-2.local:77997] mca: base: component_find: shmem "mca_shmem_sysv" uses an MCA interface that is not recognized (component MCA v2.1.0 != supported MCA v2.0.0) -- ignored
--------------------------------------------------------------------------
It looks like opal_init failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during opal_init; some of which are due to configuration or
environment problems.  This failure appears to be an internal failure;
here's some additional information (which may only be relevant to an
Open MPI developer):

  opal_shmem_base_select failed
  --> Returned value -1 instead of OPAL_SUCCESS
--------------------------------------------------------------------------
--------------------------------------------------------------------------
It looks like MPI_INIT failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during MPI_INIT; some of which are due to configuration or environment
problems.  This failure appears to be an internal failure; here's some
additional information (which may only be relevant to an Open MPI
developer):

  ompi_mpi_init: ompi_rte_init failed
  --> Returned "Error" (-1) instead of "Success" (0)
--------------------------------------------------------------------------

real	0m0.084s
user	0m0.140s
sys	0m0.132s

Testing with 11 processes
---------
[Toms-MacBook-Pro-2.local:78009] mca_base_component_repository_open: unable to open mca_grpcomm_bad: dlopen(/usr/local/lib/openmpi/mca_grpcomm_bad.so, 9): Symbol not found: _orte_grpcomm_base_modex
  Referenced from: /usr/local/lib/openmpi/mca_grpcomm_bad.so
  Expected in: flat namespace
 in /usr/local/lib/openmpi/mca_grpcomm_bad.so (ignored)
[Toms-MacBook-Pro-2.local:78009] mca_base_component_repository_open: ras "/usr/local/lib/openmpi/mca_ras_gridengine" uses an MCA interface that is not recognized (component MCA v2.0.0 != supported MCA v2.1.0) -- ignored
--------------------------------------------------------------------------
It looks like orte_init failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during orte_init; some of which are due to configuration or
environment problems.  This failure appears to be an internal failure;
here's some additional information (which may only be relevant to an
Open MPI developer):

  opal_init failed
  --> Returned value Error (-1) instead of ORTE_SUCCESS
--------------------------------------------------------------------------
*** An error occurred in MPI_Init_thread
*** on a NULL communicator
*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
***    and potentially your MPI job)
[Toms-MacBook-Pro-2.local:78010] Local abort before MPI_INIT completed successfully; not able to aggregate error messages, and not able to guarantee that all other processes were killed!
[Toms-MacBook-Pro-2.local:78010] mca: base: component_find: shmem "mca_shmem_mmap" uses an MCA interface that is not recognized (component MCA v2.1.0 != supported MCA v2.0.0) -- ignored
[Toms-MacBook-Pro-2.local:78010] mca: base: component_find: shmem "mca_shmem_posix" uses an MCA interface that is not recognized (component MCA v2.1.0 != supported MCA v2.0.0) -- ignored
[Toms-MacBook-Pro-2.local:78010] mca: base: component_find: shmem "mca_shmem_sysv" uses an MCA interface that is not recognized (component MCA v2.1.0 != supported MCA v2.0.0) -- ignored
--------------------------------------------------------------------------
It looks like opal_init failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during opal_init; some of which are due to configuration or
environment problems.  This failure appears to be an internal failure;
here's some additional information (which may only be relevant to an
Open MPI developer):

  opal_shmem_base_select failed
  --> Returned value -1 instead of OPAL_SUCCESS
--------------------------------------------------------------------------
--------------------------------------------------------------------------
It looks like MPI_INIT failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during MPI_INIT; some of which are due to configuration or environment
problems.  This failure appears to be an internal failure; here's some
additional information (which may only be relevant to an Open MPI
developer):

  ompi_mpi_init: ompi_rte_init failed
  --> Returned "Error" (-1) instead of "Success" (0)
--------------------------------------------------------------------------

real	0m0.085s
user	0m0.142s
sys	0m0.135s

Testing with 12 processes
---------
[Toms-MacBook-Pro-2.local:78025] mca_base_component_repository_open: unable to open mca_grpcomm_bad: dlopen(/usr/local/lib/openmpi/mca_grpcomm_bad.so, 9): Symbol not found: _orte_grpcomm_base_modex
  Referenced from: /usr/local/lib/openmpi/mca_grpcomm_bad.so
  Expected in: flat namespace
 in /usr/local/lib/openmpi/mca_grpcomm_bad.so (ignored)
[Toms-MacBook-Pro-2.local:78025] mca_base_component_repository_open: ras "/usr/local/lib/openmpi/mca_ras_gridengine" uses an MCA interface that is not recognized (component MCA v2.0.0 != supported MCA v2.1.0) -- ignored
--------------------------------------------------------------------------
It looks like orte_init failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during orte_init; some of which are due to configuration or
environment problems.  This failure appears to be an internal failure;
here's some additional information (which may only be relevant to an
Open MPI developer):

  opal_init failed
  --> Returned value Error (-1) instead of ORTE_SUCCESS
--------------------------------------------------------------------------
*** An error occurred in MPI_Init_thread
*** on a NULL communicator
*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
***    and potentially your MPI job)
[Toms-MacBook-Pro-2.local:78026] Local abort before MPI_INIT completed successfully; not able to aggregate error messages, and not able to guarantee that all other processes were killed!
[Toms-MacBook-Pro-2.local:78026] mca: base: component_find: shmem "mca_shmem_mmap" uses an MCA interface that is not recognized (component MCA v2.1.0 != supported MCA v2.0.0) -- ignored
[Toms-MacBook-Pro-2.local:78026] mca: base: component_find: shmem "mca_shmem_posix" uses an MCA interface that is not recognized (component MCA v2.1.0 != supported MCA v2.0.0) -- ignored
[Toms-MacBook-Pro-2.local:78026] mca: base: component_find: shmem "mca_shmem_sysv" uses an MCA interface that is not recognized (component MCA v2.1.0 != supported MCA v2.0.0) -- ignored
--------------------------------------------------------------------------
It looks like opal_init failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during opal_init; some of which are due to configuration or
environment problems.  This failure appears to be an internal failure;
here's some additional information (which may only be relevant to an
Open MPI developer):

  opal_shmem_base_select failed
  --> Returned value -1 instead of OPAL_SUCCESS
--------------------------------------------------------------------------
--------------------------------------------------------------------------
It looks like MPI_INIT failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during MPI_INIT; some of which are due to configuration or environment
problems.  This failure appears to be an internal failure; here's some
additional information (which may only be relevant to an Open MPI
developer):

  ompi_mpi_init: ompi_rte_init failed
  --> Returned "Error" (-1) instead of "Success" (0)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
It looks like orte_init failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during orte_init; some of which are due to configuration or
environment problems.  This failure appears to be an internal failure;
here's some additional information (which may only be relevant to an
Open MPI developer):

  opal_init failed
  --> Returned value Error (-1) instead of ORTE_SUCCESS
--------------------------------------------------------------------------
*** An error occurred in MPI_Init_thread
*** on a NULL communicator
*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
***    and potentially your MPI job)
[Toms-MacBook-Pro-2.local:78027] Local abort before MPI_INIT completed successfully; not able to aggregate error messages, and not able to guarantee that all other processes were killed!
[Toms-MacBook-Pro-2.local:78027] mca: base: component_find: shmem "mca_shmem_mmap" uses an MCA interface that is not recognized (component MCA v2.1.0 != supported MCA v2.0.0) -- ignored
[Toms-MacBook-Pro-2.local:78027] mca: base: component_find: shmem "mca_shmem_posix" uses an MCA interface that is not recognized (component MCA v2.1.0 != supported MCA v2.0.0) -- ignored
[Toms-MacBook-Pro-2.local:78027] mca: base: component_find: shmem "mca_shmem_sysv" uses an MCA interface that is not recognized (component MCA v2.1.0 != supported MCA v2.0.0) -- ignored
--------------------------------------------------------------------------
It looks like opal_init failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during opal_init; some of which are due to configuration or
environment problems.  This failure appears to be an internal failure;
here's some additional information (which may only be relevant to an
Open MPI developer):

  opal_shmem_base_select failed
  --> Returned value -1 instead of OPAL_SUCCESS
--------------------------------------------------------------------------
--------------------------------------------------------------------------
It looks like MPI_INIT failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during MPI_INIT; some of which are due to configuration or environment
problems.  This failure appears to be an internal failure; here's some
additional information (which may only be relevant to an Open MPI
developer):

  ompi_mpi_init: ompi_rte_init failed
  --> Returned "Error" (-1) instead of "Success" (0)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
It looks like orte_init failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during orte_init; some of which are due to configuration or
environment problems.  This failure appears to be an internal failure;
here's some additional information (which may only be relevant to an
Open MPI developer):

  opal_init failed
  --> Returned value Error (-1) instead of ORTE_SUCCESS
--------------------------------------------------------------------------
*** An error occurred in MPI_Init_thread
*** on a NULL communicator
*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
***    and potentially your MPI job)
[Toms-MacBook-Pro-2.local:78028] Local abort before MPI_INIT completed successfully; not able to aggregate error messages, and not able to guarantee that all other processes were killed!
[Toms-MacBook-Pro-2.local:78028] mca: base: component_find: shmem "mca_shmem_mmap" uses an MCA interface that is not recognized (component MCA v2.1.0 != supported MCA v2.0.0) -- ignored
[Toms-MacBook-Pro-2.local:78028] mca: base: component_find: shmem "mca_shmem_posix" uses an MCA interface that is not recognized (component MCA v2.1.0 != supported MCA v2.0.0) -- ignored
[Toms-MacBook-Pro-2.local:78028] mca: base: component_find: shmem "mca_shmem_sysv" uses an MCA interface that is not recognized (component MCA v2.1.0 != supported MCA v2.0.0) -- ignored
--------------------------------------------------------------------------
It looks like opal_init failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during opal_init; some of which are due to configuration or
environment problems.  This failure appears to be an internal failure;
here's some additional information (which may only be relevant to an
Open MPI developer):

  opal_shmem_base_select failed
  --> Returned value -1 instead of OPAL_SUCCESS
--------------------------------------------------------------------------
--------------------------------------------------------------------------
It looks like MPI_INIT failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during MPI_INIT; some of which are due to configuration or environment
problems.  This failure appears to be an internal failure; here's some
additional information (which may only be relevant to an Open MPI
developer):

  ompi_mpi_init: ompi_rte_init failed
  --> Returned "Error" (-1) instead of "Success" (0)
--------------------------------------------------------------------------

real	0m0.100s
user	0m0.210s
sys	0m0.174s

Testing with 12 processes
---------
[Toms-MacBook-Pro-2.local:78039] mca_base_component_repository_open: unable to open mca_grpcomm_bad: dlopen(/usr/local/lib/openmpi/mca_grpcomm_bad.so, 9): Symbol not found: _orte_grpcomm_base_modex
  Referenced from: /usr/local/lib/openmpi/mca_grpcomm_bad.so
  Expected in: flat namespace
 in /usr/local/lib/openmpi/mca_grpcomm_bad.so (ignored)
[Toms-MacBook-Pro-2.local:78039] mca_base_component_repository_open: ras "/usr/local/lib/openmpi/mca_ras_gridengine" uses an MCA interface that is not recognized (component MCA v2.0.0 != supported MCA v2.1.0) -- ignored
--------------------------------------------------------------------------
It looks like orte_init failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during orte_init; some of which are due to configuration or
environment problems.  This failure appears to be an internal failure;
here's some additional information (which may only be relevant to an
Open MPI developer):

  opal_init failed
  --> Returned value Error (-1) instead of ORTE_SUCCESS
--------------------------------------------------------------------------
*** An error occurred in MPI_Init_thread
*** on a NULL communicator
*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
***    and potentially your MPI job)
[Toms-MacBook-Pro-2.local:78040] Local abort before MPI_INIT completed successfully; not able to aggregate error messages, and not able to guarantee that all other processes were killed!
[Toms-MacBook-Pro-2.local:78040] mca: base: component_find: shmem "mca_shmem_mmap" uses an MCA interface that is not recognized (component MCA v2.1.0 != supported MCA v2.0.0) -- ignored
[Toms-MacBook-Pro-2.local:78040] mca: base: component_find: shmem "mca_shmem_posix" uses an MCA interface that is not recognized (component MCA v2.1.0 != supported MCA v2.0.0) -- ignored
[Toms-MacBook-Pro-2.local:78040] mca: base: component_find: shmem "mca_shmem_sysv" uses an MCA interface that is not recognized (component MCA v2.1.0 != supported MCA v2.0.0) -- ignored
--------------------------------------------------------------------------
It looks like opal_init failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during opal_init; some of which are due to configuration or
environment problems.  This failure appears to be an internal failure;
here's some additional information (which may only be relevant to an
Open MPI developer):

  opal_shmem_base_select failed
  --> Returned value -1 instead of OPAL_SUCCESS
--------------------------------------------------------------------------
--------------------------------------------------------------------------
It looks like MPI_INIT failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during MPI_INIT; some of which are due to configuration or environment
problems.  This failure appears to be an internal failure; here's some
additional information (which may only be relevant to an Open MPI
developer):

  ompi_mpi_init: ompi_rte_init failed
  --> Returned "Error" (-1) instead of "Success" (0)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
It looks like orte_init failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during orte_init; some of which are due to configuration or
environment problems.  This failure appears to be an internal failure;
here's some additional information (which may only be relevant to an
Open MPI developer):

  opal_init failed
  --> Returned value Error (-1) instead of ORTE_SUCCESS
--------------------------------------------------------------------------
*** An error occurred in MPI_Init_thread
*** on a NULL communicator
*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
***    and potentially your MPI job)
[Toms-MacBook-Pro-2.local:78041] Local abort before MPI_INIT completed successfully; not able to aggregate error messages, and not able to guarantee that all other processes were killed!
[Toms-MacBook-Pro-2.local:78041] mca: base: component_find: shmem "mca_shmem_mmap" uses an MCA interface that is not recognized (component MCA v2.1.0 != supported MCA v2.0.0) -- ignored
[Toms-MacBook-Pro-2.local:78041] mca: base: component_find: shmem "mca_shmem_posix" uses an MCA interface that is not recognized (component MCA v2.1.0 != supported MCA v2.0.0) -- ignored
[Toms-MacBook-Pro-2.local:78041] mca: base: component_find: shmem "mca_shmem_sysv" uses an MCA interface that is not recognized (component MCA v2.1.0 != supported MCA v2.0.0) -- ignored
--------------------------------------------------------------------------
It looks like opal_init failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during opal_init; some of which are due to configuration or
environment problems.  This failure appears to be an internal failure;
here's some additional information (which may only be relevant to an
Open MPI developer):

  opal_shmem_base_select failed
  --> Returned value -1 instead of OPAL_SUCCESS
--------------------------------------------------------------------------
--------------------------------------------------------------------------
It looks like MPI_INIT failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during MPI_INIT; some of which are due to configuration or environment
problems.  This failure appears to be an internal failure; here's some
additional information (which may only be relevant to an Open MPI
developer):

  ompi_mpi_init: ompi_rte_init failed
  --> Returned "Error" (-1) instead of "Success" (0)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
It looks like orte_init failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during orte_init; some of which are due to configuration or
environment problems.  This failure appears to be an internal failure;
here's some additional information (which may only be relevant to an
Open MPI developer):

  opal_init failed
  --> Returned value Error (-1) instead of ORTE_SUCCESS
--------------------------------------------------------------------------
*** An error occurred in MPI_Init_thread
*** on a NULL communicator
*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
***    and potentially your MPI job)
[Toms-MacBook-Pro-2.local:78042] Local abort before MPI_INIT completed successfully; not able to aggregate error messages, and not able to guarantee that all other processes were killed!
[Toms-MacBook-Pro-2.local:78042] mca: base: component_find: shmem "mca_shmem_mmap" uses an MCA interface that is not recognized (component MCA v2.1.0 != supported MCA v2.0.0) -- ignored
[Toms-MacBook-Pro-2.local:78042] mca: base: component_find: shmem "mca_shmem_posix" uses an MCA interface that is not recognized (component MCA v2.1.0 != supported MCA v2.0.0) -- ignored
[Toms-MacBook-Pro-2.local:78042] mca: base: component_find: shmem "mca_shmem_sysv" uses an MCA interface that is not recognized (component MCA v2.1.0 != supported MCA v2.0.0) -- ignored
--------------------------------------------------------------------------
It looks like opal_init failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during opal_init; some of which are due to configuration or
environment problems.  This failure appears to be an internal failure;
here's some additional information (which may only be relevant to an
Open MPI developer):

  opal_shmem_base_select failed
  --> Returned value -1 instead of OPAL_SUCCESS
--------------------------------------------------------------------------
--------------------------------------------------------------------------
It looks like MPI_INIT failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during MPI_INIT; some of which are due to configuration or environment
problems.  This failure appears to be an internal failure; here's some
additional information (which may only be relevant to an Open MPI
developer):

  ompi_mpi_init: ompi_rte_init failed
  --> Returned "Error" (-1) instead of "Success" (0)
--------------------------------------------------------------------------

real	0m0.106s
user	0m0.227s
sys	0m0.178s

Testing with 12 processes
---------
[Toms-MacBook-Pro-2.local:78053] mca_base_component_repository_open: unable to open mca_grpcomm_bad: dlopen(/usr/local/lib/openmpi/mca_grpcomm_bad.so, 9): Symbol not found: _orte_grpcomm_base_modex
  Referenced from: /usr/local/lib/openmpi/mca_grpcomm_bad.so
  Expected in: flat namespace
 in /usr/local/lib/openmpi/mca_grpcomm_bad.so (ignored)
[Toms-MacBook-Pro-2.local:78053] mca_base_component_repository_open: ras "/usr/local/lib/openmpi/mca_ras_gridengine" uses an MCA interface that is not recognized (component MCA v2.0.0 != supported MCA v2.1.0) -- ignored
--------------------------------------------------------------------------
It looks like orte_init failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during orte_init; some of which are due to configuration or
environment problems.  This failure appears to be an internal failure;
here's some additional information (which may only be relevant to an
Open MPI developer):

  opal_init failed
  --> Returned value Error (-1) instead of ORTE_SUCCESS
--------------------------------------------------------------------------
*** An error occurred in MPI_Init_thread
*** on a NULL communicator
*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
***    and potentially your MPI job)
[Toms-MacBook-Pro-2.local:78054] Local abort before MPI_INIT completed successfully; not able to aggregate error messages, and not able to guarantee that all other processes were killed!
[Toms-MacBook-Pro-2.local:78054] mca: base: component_find: shmem "mca_shmem_mmap" uses an MCA interface that is not recognized (component MCA v2.1.0 != supported MCA v2.0.0) -- ignored
[Toms-MacBook-Pro-2.local:78054] mca: base: component_find: shmem "mca_shmem_posix" uses an MCA interface that is not recognized (component MCA v2.1.0 != supported MCA v2.0.0) -- ignored
[Toms-MacBook-Pro-2.local:78054] mca: base: component_find: shmem "mca_shmem_sysv" uses an MCA interface that is not recognized (component MCA v2.1.0 != supported MCA v2.0.0) -- ignored
--------------------------------------------------------------------------
It looks like opal_init failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during opal_init; some of which are due to configuration or
environment problems.  This failure appears to be an internal failure;
here's some additional information (which may only be relevant to an
Open MPI developer):

  opal_shmem_base_select failed
  --> Returned value -1 instead of OPAL_SUCCESS
--------------------------------------------------------------------------
--------------------------------------------------------------------------
It looks like MPI_INIT failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during MPI_INIT; some of which are due to configuration or environment
problems.  This failure appears to be an internal failure; here's some
additional information (which may only be relevant to an Open MPI
developer):

  ompi_mpi_init: ompi_rte_init failed
  --> Returned "Error" (-1) instead of "Success" (0)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
It looks like orte_init failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during orte_init; some of which are due to configuration or
environment problems.  This failure appears to be an internal failure;
here's some additional information (which may only be relevant to an
Open MPI developer):

  opal_init failed
  --> Returned value Error (-1) instead of ORTE_SUCCESS
--------------------------------------------------------------------------
*** An error occurred in MPI_Init_thread
*** on a NULL communicator
*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
***    and potentially your MPI job)
[Toms-MacBook-Pro-2.local:78055] Local abort before MPI_INIT completed successfully; not able to aggregate error messages, and not able to guarantee that all other processes were killed!
[Toms-MacBook-Pro-2.local:78055] mca: base: component_find: shmem "mca_shmem_mmap" uses an MCA interface that is not recognized (component MCA v2.1.0 != supported MCA v2.0.0) -- ignored
[Toms-MacBook-Pro-2.local:78055] mca: base: component_find: shmem "mca_shmem_posix" uses an MCA interface that is not recognized (component MCA v2.1.0 != supported MCA v2.0.0) -- ignored
[Toms-MacBook-Pro-2.local:78055] mca: base: component_find: shmem "mca_shmem_sysv" uses an MCA interface that is not recognized (component MCA v2.1.0 != supported MCA v2.0.0) -- ignored
--------------------------------------------------------------------------
It looks like opal_init failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during opal_init; some of which are due to configuration or
environment problems.  This failure appears to be an internal failure;
here's some additional information (which may only be relevant to an
Open MPI developer):

  opal_shmem_base_select failed
  --> Returned value -1 instead of OPAL_SUCCESS
--------------------------------------------------------------------------
--------------------------------------------------------------------------
It looks like MPI_INIT failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during MPI_INIT; some of which are due to configuration or environment
problems.  This failure appears to be an internal failure; here's some
additional information (which may only be relevant to an Open MPI
developer):

  ompi_mpi_init: ompi_rte_init failed
  --> Returned "Error" (-1) instead of "Success" (0)
--------------------------------------------------------------------------

real	0m0.092s
user	0m0.188s
sys	0m0.154s

Testing with local, non-distributed solver
---------

real	0m0.072s
user	0m0.013s
sys	0m0.020s
